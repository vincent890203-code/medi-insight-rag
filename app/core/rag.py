import os
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain
from langchain_core.documents import Document

load_dotenv()

def start_chat():
    print("ğŸ§  æ­£åœ¨å•Ÿå‹• Medi-Insight RAG ç³»çµ± (æœ¬åœ°ç©©å®šç‰ˆ)...")
    embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
    
    # å»ºç«‹ç¯„ä¾‹çŸ¥è­˜åº«
    docs = [Document(page_content="ç—…äººå¼µä¸‰ï¼ŒEGFR L858R çªè®Šé™½æ€§ï¼Œå»ºè­°ä½¿ç”¨ Osimertinibã€‚")]
    vector_store = FAISS.from_documents(docs, embeddings)
    retriever = vector_store.as_retriever()

    llm = ChatGoogleGenerativeAI(model="models/gemini-flash-latest", temperature=0)
    prompt = ChatPromptTemplate.from_template("æ ¹æ“šå…§å®¹å›ç­”ï¼š{context}\nå•é¡Œï¼š{input}")

    # æ ¸å¿ƒè™•ç†éˆ
    qa_chain = create_stuff_documents_chain(llm, prompt)
    rag_chain = create_retrieval_chain(retriever, qa_chain)

    print("âœ… ç³»çµ±å°±ç·’ï¼")
    while True:
        user_input = input("\nğŸ‘¨â€âš•ï¸ é†«å¸«æå•: ")
        if user_input.lower() in ['q', 'exit']: break
        res = rag_chain.invoke({"input": user_input})
        print(f"\nğŸ“ AI è¨ºæ–·ï¼š{res['answer']}")

if __name__ == "__main__":
    start_chat()