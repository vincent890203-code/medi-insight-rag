import os
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain
from langchain_core.documents import Document

# 1. è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

def initialize_rag_system():
    print("ğŸ§  æ­£åœ¨å•Ÿå‹• Medi-Insight RAG ç³»çµ± (æœ¬åœ°ç©©å®šç‰ˆ)...")

    # 2. æº–å‚™ Embeddings (å…¨åŸŸè®Šæ•¸)
    embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
    
  # 3. è¼‰å…¥å‘é‡è³‡æ–™åº« (é—œéµä¿®æ”¹ï¼)
    DB_PATH = "faiss_index" # è³‡æ–™åº«è·¯å¾‘
    
    if os.path.exists(DB_PATH):
        print(f"ğŸ“‚ ç™¼ç¾æœ¬åœ°è³‡æ–™åº«ï¼Œæ­£åœ¨è¼‰å…¥: {DB_PATH}")
        # allow_dangerous_deserialization=True æ˜¯å¿…é ˆçš„
        # å› ç‚º FAISS è®€å– pickle æª”æœ‰å®‰å…¨é¢¨éšªï¼Œä½†é€™æ˜¯æˆ‘å€‘è‡ªå·±ç”Ÿæˆçš„æª”ï¼Œæ‰€ä»¥å®‰å…¨
        vector_store = FAISS.load_local(
            DB_PATH, 
            embeddings, 
            allow_dangerous_deserialization=True
        )
    else:
        print("âš ï¸ è­¦å‘Šï¼šæ‰¾ä¸åˆ° faiss_index è³‡æ–™å¤¾ï¼")
        print("ğŸ’¡ è«‹å…ˆåŸ·è¡Œ 'python app/core/ingest.py' ä¾†æ¶ˆåŒ– PDFã€‚")
        # è¬ä¸€çœŸçš„æ²’æª”æ¡ˆï¼Œçµ¦å€‹ç©ºæ®¼é¿å…ç¨‹å¼å´©æ½°
        return None

    # 4. å»ºç«‹æª¢ç´¢å™¨ (Retriever)
    retriever = vector_store.as_retriever()

    # 5. è¨­å®š LLM (ä½¿ç”¨æˆ‘å€‘ç¢ºèªéå¯ç”¨çš„æ¨¡å‹)
    llm = ChatGoogleGenerativeAI(model="models/gemini-flash-latest", temperature=0)

    # 6. è¨­å®š Prompt Template
    prompt = ChatPromptTemplate.from_template("""
    ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„é†«ç™‚ AI åŠ©ç†ã€‚è«‹æ ¹æ“šåº•ä¸‹çš„ã€ç—…æ­·æ‘˜è¦ã€‘ä¾†å›ç­”é†«å¸«çš„å•é¡Œã€‚
    å¦‚æœä¸ç¢ºå®šæˆ–è³‡æ–™ä¸åœ¨æ‘˜è¦ä¸­ï¼Œè«‹å›ç­”ã€Œç—…æ­·ä¸­æœªæåŠã€ã€‚

    ã€ç—…æ­·æ‘˜è¦ã€‘ï¼š
    {context}

    å•é¡Œï¼š{input}
    """)

    # æ ¸å¿ƒè™•ç†éˆ
    qa_chain = create_stuff_documents_chain(llm, prompt)
    rag_chain = create_retrieval_chain(retriever, qa_chain)

    print("âœ… RAGæ ¸å¿ƒç³»çµ±å°±ç·’ï¼")

    # ã€é—œéµä¿®æ”¹ã€‘å¿…é ˆæŠŠåšå¥½çš„éŠå‚³å‡ºå»ï¼Œç¶²é æ‰æ‹¿å¾—åˆ°
    return rag_chain

# --- é€™æ˜¯çµ¦çµ‚ç«¯æ©Ÿæ¸¬è©¦ç”¨çš„å‡½å¼ ---
def start_terminal_chat():
    # åœ¨é€™è£¡å‘¼å«åˆå§‹åŒ–å‡½å¼
    rag_chain = initialize_rag_system()

    print("ğŸš€ å•Ÿå‹•çµ‚ç«¯æ©Ÿå°è©±æ¨¡å¼...")    
    while True:
        try:
            user_input = input("\nğŸ‘¨â€âš•ï¸ é†«å¸«æå•(è¼¸å…¥ q é›¢é–‹): ")
            if user_input.lower() in ['q', 'exit']: 
                print("å†è¦‹!")
                break
        
            res = rag_chain.invoke({"input": user_input})
            print(f"\nğŸ“ AI è¨ºæ–·ï¼š{res['answer']}")
        except Exception as e:
            print(f"âŒ ç™¼ç”ŸéŒ¯èª¤: {e}")


# --- ç¨‹å¼é€²å…¥é»ä¿è­· ---
# åªæœ‰ç›´æ¥åŸ·è¡Œé€™å€‹æª”æ¡ˆæ™‚ï¼Œæ‰æœƒè·‘çµ‚ç«¯æ©Ÿå°è©±
# å¦‚æœæ˜¯è¢« web_ui.py åŒ¯å…¥ (import)ï¼Œé€™æ®µä¸æœƒè·‘ï¼Œé¿å…å¡æ­»ç¶²é 
if __name__ == "__main__":
    start_terminal_chat()