import os
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain

# 1. è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# å…¨åŸŸè®Šæ•¸ (Singleton Pattern)
vector_store = None
llm = None
embeddings = None

def initialize_rag_components():
    """åˆå§‹åŒ–æ ¸å¿ƒçµ„ä»¶ (åªåŸ·è¡Œä¸€æ¬¡)"""
    global vector_store, llm, embeddings
    
    if vector_store is not None:
        return # å·²ç¶“åˆå§‹åŒ–éï¼Œç›´æ¥è·³é

    print("æ­£åœ¨åˆå§‹åŒ– Medi-Insight RAG çµ„ä»¶ ...")
    
    # æº–å‚™ Embeddings
    embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
    
    # è¼‰å…¥å‘é‡è³‡æ–™åº«
    DB_PATH = "faiss_index"
    if os.path.exists(DB_PATH):
        print(f"ğŸ“‚ è¼‰å…¥æœ¬åœ°è³‡æ–™åº«: {DB_PATH}")
        vector_store = FAISS.load_local(
            DB_PATH, 
            embeddings, 
            allow_dangerous_deserialization=True
        )
    else:
        print("âš ï¸ è­¦å‘Šï¼šæ‰¾ä¸åˆ° faiss_index è³‡æ–™å¤¾ï¼è«‹å…ˆåŸ·è¡Œ ingestã€‚")
        return

    # è¨­å®š LLM
    llm = ChatGoogleGenerativeAI(model="models/gemini-flash-latest", temperature=0)
    print("âœ… RAG çµ„ä»¶åˆå§‹åŒ–å®Œæˆï¼")

def get_rag_chain(selected_source=None):
    """
    å‹•æ…‹å»ºç«‹ RAG Chain
    :param selected_source: å®Œæ•´æª”æ¡ˆè·¯å¾‘ (ä¾‹å¦‚ 'data/patient_report_002.pdf')
    """
    # ç¢ºä¿çµ„ä»¶å·²åˆå§‹åŒ–
    if vector_store is None:
        initialize_rag_components()
        if vector_store is None: return None # çœŸçš„æ²’æ•‘äº†

    # 1. è¨­å®šæª¢ç´¢å™¨ (Retriever) èˆ‡éæ¿¾å™¨
    search_kwargs = {"k": 3}
    
    if selected_source:
        # ğŸ’¡ é—œéµï¼šMetadata Filtering
        # å‘Šè¨´ FAISS åªæœå°‹ source æ¬„ä½ç­‰æ–¼ selected_source çš„å‘é‡
        search_kwargs["filter"] = {"source": selected_source}
        print(f"ğŸ” [RAG] å•Ÿç”¨éæ¿¾æ¨¡å¼: åªæœå°‹ {selected_source}")
    else:
        print("ğŸ” [RAG] å…¨åŸŸæœå°‹æ¨¡å¼ (æœå°‹æ‰€æœ‰ç—…æ­·)")

    retriever = vector_store.as_retriever(search_kwargs=search_kwargs)

    # 2. è¨­å®š Prompt
    prompt = ChatPromptTemplate.from_template("""
    ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„é†«ç™‚ AI åŠ©ç†ã€‚è«‹æ ¹æ“šåº•ä¸‹çš„ã€ç—…æ­·æ‘˜è¦ã€‘ä¾†å›ç­”é†«å¸«çš„å•é¡Œã€‚
    æ³¨æ„ï¼šä½ åªèƒ½å›ç­”èˆ‡è©²ç—…æ­·ç›¸é—œçš„è³‡è¨Šã€‚
    å¦‚æœä¸ç¢ºå®šæˆ–è³‡æ–™ä¸åœ¨æ‘˜è¦ä¸­ï¼Œè«‹å›ç­”ã€Œç—…æ­·ä¸­æœªæåŠã€ã€‚

    ã€ç—…æ­·æ‘˜è¦ã€‘ï¼š
    {context}

    å•é¡Œï¼š{input}
    """)

    # 3. çµ„åˆ Chain
    qa_chain = create_stuff_documents_chain(llm, prompt)
    rag_chain = create_retrieval_chain(retriever, qa_chain)
    
    return rag_chain